{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Project 2\n",
    "<a href=\"https://colab.research.google.com/github/BolachinhaAmericana/BigDataProject/blob/main/colab-code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "![](./images/ESTB.png)\\\n",
    "ESTBarreiro - IPS\\\n",
    "Bioinformatics, Janurry 2023\\\n",
    "3rd year, 1st semester\\\n",
    "Made by:\n",
    "- Duarte Valente 202000053\n",
    "- Guilherme Sá 202000201 \\\n",
    "Teachers:\n",
    "- Raquel Barreira\n",
    "- António Gonçalves\n",
    "\n",
    "# Introduction\n",
    "This project has the main purpose getting us to know and use Spark on a Machine Learning context. \\\n",
    "Here we are going to use machine learning algorithms to analyze a big amount of data, predict results to a problem as well as use Spark to read and present the results regarding the ML models.\\\n",
    "All of this has the intent of creating a big data and high performance environment.\n",
    "## Dataset Introduction\n",
    "The choosen dataset can be found [here](https://www.kaggle.com/code/ukveteran/sparkml-credit-card-fraud-jma/data).\\\n",
    "This data contains information about credit card transactions where we will focus on trying to predict weather a transaction is fraud or not based on what transactions characteristics we have.\\\n",
    "- Dataset Name: SparkML - Credit Card Fraud - KMA\n",
    "- Dataset file name: creditcard.csv\n",
    "# Code\n",
    "## Installing necessary packages\n",
    "```python\n",
    "%pip install pyspark\n",
    "\n",
    "# Spark Session imports\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Spark ML imports\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# other imports \n",
    "import matplotlib as plt\n",
    "```\n",
    "## Creating a Spark environment\n",
    "Here we are going to create a pyspark environment called: \"CreditcardPrediction\" using the local machine as master.\\\n",
    "\\\n",
    "When we are working with Spark, we need to create a session SparkSession to manage all configurations on our spark environment.\n",
    "```python\n",
    "conf= SparkConf().setAppName('CreditCardPrediction').setMaster('local[*]')\n",
    "sc= SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "```\n",
    "## Visualizing Data\n",
    "Taking a look at a sample from the data:\n",
    "```python\n",
    "df.toPandas().head()\n",
    "```\n",
    "![](./images/Dataset-head.png)\n",
    "## Dataset Columns description\n",
    "The data is composed of 31 columns and 28 of them are ``` Vx```. These go from ```V1``` all the way to ```V28``` and represent encrypted characteristics to protect users information.\n",
    "- ```Time``` -> Number of seconds between current and first transactions.\n",
    "- ```V(1-28)``` -> encrypted user information.\n",
    "- ```Amount``` -> Amount of money transferred\n",
    "- ```Class``` -> Classifier if transaction is fraud. Will be 0 if fraud and 0 otherwise\n",
    "### Data Distribution\n",
    "```python\n",
    "# code\n",
    "counts = df.groupBy(\"Class\").count()\n",
    "counts.show()\n",
    "\n",
    "#output\n",
    "+-------+--------+\n",
    "| Class | count  |\n",
    "+=======+========+\n",
    "|    1  |   492  |\n",
    "|    0  | 284315 |\n",
    "+-------+--------+\n",
    "# code\n",
    "print(f\"Numero de Linhas: {df.count()}\")\n",
    "print(f\"Numero de Colunas:{len(df.columns)}\")\n",
    "print(\"Esquema do DataFrame:\")\n",
    "df.printSchema()\n",
    "# output\n",
    "Numero de Linhas: 284807\n",
    "Numero de Colunas:31\n",
    "Esquema do DataFrame:\n",
    "root\n",
    " |-- Time: double (nullable = true)\n",
    " |-- V1: double (nullable = true)\n",
    " |-- V2: double (nullable = true)\n",
    " |-- V3: double (nullable = true)\n",
    " |-- V4: double (nullable = true)\n",
    " |-- V5: double (nullable = true)\n",
    " |-- V6: double (nullable = true)\n",
    " |-- V7: double (nullable = true)\n",
    " |-- V8: double (nullable = true)\n",
    " |-- V9: double (nullable = true)\n",
    " |-- V10: double (nullable = true)\n",
    " |-- V11: double (nullable = true)\n",
    " |-- V12: double (nullable = true)\n",
    " |-- V13: double (nullable = true)\n",
    " |-- V14: double (nullable = true)\n",
    " |-- V15: double (nullable = true)\n",
    " |-- V16: double (nullable = true)\n",
    " |-- V17: double (nullable = true)\n",
    " |-- V18: double (nullable = true)\n",
    " |-- V19: double (nullable = true)\n",
    " |-- V20: double (nullable = true)\n",
    " |-- V21: double (nullable = true)\n",
    " |-- V22: double (nullable = true)\n",
    " |-- V23: double (nullable = true)\n",
    " |-- V24: double (nullable = true)\n",
    " |-- V25: double (nullable = true)\n",
    " |-- V26: double (nullable = true)\n",
    " |-- V27: double (nullable = true)\n",
    " |-- V28: double (nullable = true)\n",
    " |-- Amount: double (nullable = true)\n",
    " |-- Class: integer (nullable = true)\n",
    "```\n",
    "## Summarizing Data\n",
    "The data has a dimension of 284804 entities and 31 characteristics.\\\n",
    "It also has a tiny amount of **fraud** cases (492), around **0.2%** of the **sample size**.\\\n",
    "Given that this is comes off as a **yes** or **no** question we are expecting some problems on training our prediction models given this disparity in value samples.\n",
    "## Graphs\n",
    "Code to graphs to all characteristics and their distribution\n",
    "```python\n",
    "num_cols = [i for i in df.columns]\n",
    "df_num = df.select(num_cols)\n",
    "df_num.toPandas().hist(figsize=[15,15],bins=30, color='orange', histtype='bar', ec='black')\n",
    "```\n",
    "\\\n",
    "\\\n",
    "![](./images/histograms.png)\n",
    "### Brief analysis of the histograms\n",
    "These histograms are presented with the intent of showing the distribution of the count on the values of their range.\\\n",
    "- ```Time``` -> There is not much to take off this histogram other then the fact that there was no relevant time gap where there where no transactions. We can observe, however that there are peak times for sure.\n",
    "- ```V(1-28)``` -> We can't take any valuable insights from this characteristics since we don't know what are they referring to.\n",
    "- ```Amount``` -> We can observe that most of the transfer amount goes a tiny bit to the right of 0 and that almost all transactions are of values under 1000.\n",
    "- ```Class``` -> On this histogram we can visually confirm that the disparity in the amount of non-fraud transactions is absolutely dominating and that we can't even see the fraud transactions bar.\n",
    "## Preparing data to apply machine learning\n",
    "We are start by verifying the existence of empty values:\n",
    "```python\n",
    "# code\n",
    "null_values = []\n",
    "for col in df.columns:\n",
    "    null_values.append((col, df.filter(df[col].isNull()).count()))\n",
    "col_count = 0\n",
    "null_col_count = 0\n",
    "for col, value in null_values:\n",
    "    col_count = col_count + 1\n",
    "    if value == 0:\n",
    "        null_col_count = null_col_count + 1\n",
    "    else:\n",
    "        print(f\"{col} : {value} missing values\")\n",
    "if col_count == null_col_count:\n",
    "    print(\"Nao ha valores nulos\")\n",
    "# output on this dataset\n",
    "Nao ha valores nulos\n",
    "```\n",
    "After verifying that we don't have any null values, we are going to transform the entries into vectors.\n",
    "> To do this we are going to gather all characteristics from the data and exclude the target variable to create the vector object with all information but the one we cut off. After this our result should be something like:\\\n",
    "```[feature 1 | col1 | col2 | col... | Class]``` to:\\\n",
    "```[col1 | Class]```\n",
    "```python\n",
    "#code\n",
    "cols = df.columns\n",
    "cols.remove('Class')\n",
    "# first we add al columns to a var and remove last col\n",
    "assembler = VectorAssembler(inputCols= cols, outputCol= 'features')\n",
    "# second we \"glue\" all the features together inside this variable.\n",
    "df = assembler.transform(df)\n",
    "```\n",
    "after this we visualize the data after the changed and we should be able to observe that instead of 31 columns we now have only 2 and we will be using the \"features\" column to predict the \"Class\" since \"features\" is nothing more nothing less then all the other 30 features mashed together.\n",
    "```python\n",
    "#code\n",
    "df = df.select(['features', 'Class'])\n",
    "df.show(5)\n",
    "#output\n",
    "+--------------------+-----+\n",
    "|            features|Class|\n",
    "+--------------------+-----+\n",
    "|[0.0,-1.359807133...|    0|\n",
    "|[0.0,1.1918571113...|    0|\n",
    "|[1.0,-1.358354061...|    0|\n",
    "|[1.0,-0.966271711...|    0|\n",
    "|[2.0,-1.158233093...|    0|\n",
    "+--------------------+-----+\n",
    "only showing top 5 rows\n",
    "```\n",
    "## Applying Machine Learning Models\n",
    "In this project we will be using 2 ML algorithms, *Linear Regression* and *Random Forest*.\\\n",
    "The choice of these models was made based on the fact that these algorithms are great to binary classification therefor are adequate for our dataset.\n",
    "### Defining Test/Split variables.\n",
    "In this case we are going to split the data as of 70% to training and 30% to test. We are also going to leave the seed parameter empty in order for the data cut to be made randomly every time.\n",
    "```python\n",
    "training, testing= df.randomSplit([0.7, 0.3])\n",
    "```\n",
    "### Model -> Logistic Regression\n",
    "#### Training and fitting the model\n",
    "```python\n",
    "lr = LogisticRegression(labelCol='Class', featuresCol='features')\n",
    "model= lr.fit(training) # fit training data\n",
    "predictions = model.transform(testing) # testing the model with test data\n",
    "```\n",
    "#### Verifying test result\n",
    "```python\n",
    "#code\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Class', rawPredictionCol='rawPrediction')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "#output of a random run\n",
    "0.960957953931887\n",
    "```\n",
    "> With this we can observe that with logistic regression we got an accuracy of approximately 96.1%\n",
    "### Model -> Random Forest\n",
    "#### Training and fitting the model\n",
    "We decided to make the tree max depth of 5 to avoid overfitting.\n",
    "```python\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", maxDepth= 5)\n",
    "model = rf.fit(training)\n",
    "predictions = model.transform(testing)\n",
    "```\n",
    "#### Verifying test result\n",
    "```python\n",
    "#code\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Class', rawPredictionCol='rawPrediction')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy\n",
    "#output of a random run\n",
    "0.9621722395163965\n",
    "```\n",
    "> With this we can observe that with Random Forest we got an accuracy of approximately 96.2%\n",
    "# Conclusion\n",
    "In this work we used PySpark to load, process and analyze a Dataset while simulating a BigData and High-Performance environment.\\\n",
    "Following this we used MLlib to train and evaluate multiple machine leaning models such as Logistic regression and Random Forest.\\\n",
    "Both of these models got a performance that we though was adequate on the matter that they had good accuracy values despite the huge discrepancy on the classes sample size variety matter.\n",
    "\n",
    "**Should be noted**\\\n",
    "Logistic Regression we got a lower value then on Random Forest algorithm. This was due to to random chance on part of the test-split division and most of the times we ran the program we verified that the Logistic Regression usually displays the best results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
